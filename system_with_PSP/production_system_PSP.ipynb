{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Production System Assignment",
   "id": "85d2331b4501b10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Production System with PSP and RL agent optimization",
   "id": "85482f5a8063c4f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T09:08:08.950854Z",
     "start_time": "2025-06-29T09:08:01.302760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import statistics\n",
    "from collections.abc import Callable, Sequence\n",
    "\n",
    "import joblib.externals.cloudpickle\n",
    "import numpy as np\n",
    "import simpy\n",
    "from scipy import stats\n",
    "from simpy.events import ProcessGenerator\n",
    "from lib.server import Server\n",
    "from lib.job import Job\n",
    "from matplotlib import pyplot as plt\n",
    "from lib.config import SEEDS\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.running_mean_std import RunningMeanStd\n",
    "import os"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T10:01:49.081125Z",
     "start_time": "2025-06-29T10:01:49.035304Z"
    }
   },
   "cell_type": "code",
   "source": "NUM_MACHINES = 6",
   "id": "5fa4fdefce1ada54",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SystemEnv(gym.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inter_arrival_time_distribution: Callable[[], float],\n",
    "        processing_time_per_family_distribution: list[Callable[[], float]],\n",
    "        families_distribution: Callable[[], float],\n",
    "        due_dates_distribution: Callable[[], float],\n",
    "        routing_distribution: dict[int, list[Callable[[], float]]],\n",
    "        routing_prob: dict[int, list[float]],\n",
    "        agent_decision_interval: float = 3.0, # 10, 5\n",
    "        episode_duration: float = 60 * 160,\n",
    "        reward_weights: dict[str, float] = None\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.env: simpy.Environment | None = None\n",
    "        self.inter_arrival_time_distribution = inter_arrival_time_distribution\n",
    "        self.processing_time_per_family_distribution = processing_time_per_family_distribution\n",
    "        self.families_distribution = families_distribution\n",
    "        self.due_dates_distribution = due_dates_distribution\n",
    "        self.routing_distribution = routing_distribution\n",
    "        self.routing_prob = routing_prob\n",
    "        self.agent_decision_interval = agent_decision_interval\n",
    "        self.episode_duration = episode_duration\n",
    "\n",
    "        self.running_throughput = RunningMeanStd()\n",
    "        self.running_tardiness = RunningMeanStd()\n",
    "        self.running_wip = RunningMeanStd()\n",
    "\n",
    "        self.reward_weights = reward_weights if reward_weights is not None else {\n",
    "            'throughput': 1.0,\n",
    "            'wip_penalty': -1.0,\n",
    "            'tardiness_penalty': -1.0,\n",
    "            'earliness_penalty': -1.0\n",
    "        }\n",
    "\n",
    "        low_bounds = np.array([\n",
    "            0.0,\n",
    "            0.0,\n",
    "            -5000.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        high_bounds = np.array([\n",
    "            1.0,\n",
    "            500.0,\n",
    "            5000.0,\n",
    "            1000.0,\n",
    "            10000.0,\n",
    "            10000.0,\n",
    "            10000.0,\n",
    "            10000.0,\n",
    "            10000.0,\n",
    "            10000.0\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        self.observation_space = spaces.Box(low=low_bounds, high=high_bounds, dtype=np.float32)\n",
    "\n",
    "        # action space: 0 = Don't Release, 1 = Release\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.machines: list[Server] = []\n",
    "        self.jobs: list[Job] = []\n",
    "        self.pre_shop_pool: list[Job] = []\n",
    "        self.idx_counter = 0\n",
    "\n",
    "        self.jobs_completed_this_episode = 0\n",
    "        self.total_tardiness_this_episode = 0.0\n",
    "        self.total_earliness_this_episode = 0.0\n",
    "\n",
    "        self.last_wip_value = 0\n",
    "        self.last_wip_time = 0.0\n",
    "        self.cumulative_wip_area = 0.0\n",
    "        self.current_interval_wip_area = 0.0\n",
    "\n",
    "        self.next_scheduled_arrival_time = float('inf')\n",
    "\n",
    "    def _update_wip_area(self):\n",
    "        now = self.env.now\n",
    "        duration = now - self.last_wip_time\n",
    "        if duration > 0:\n",
    "            self.cumulative_wip_area += self.last_wip_value * duration\n",
    "            self.current_interval_wip_area += self.last_wip_value * duration\n",
    "        self.last_wip_time = now\n",
    "        self.last_wip_value = sum([j.remaining_processing_time for j in self.jobs if j.done is not True])\n",
    "\n",
    "    def _set_seed(self, seed: int | None = None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def reset(self, seed: int | None = None, options: dict | None = None) -> tuple[np.ndarray, dict]:\n",
    "        super().reset(seed=seed)\n",
    "        self._set_seed(seed)\n",
    "\n",
    "        self.env = simpy.Environment()\n",
    "        self.machines = [Server(self.env, 1, f\"WC{i+1}\") for i in range(6)]\n",
    "        self.pre_shop_pool = []\n",
    "        self.jobs = []\n",
    "\n",
    "        self.idx_counter = 0\n",
    "\n",
    "        self.jobs_completed_this_episode = 0\n",
    "        self.total_tardiness_this_episode = 0.0\n",
    "        self.total_earliness_this_episode = 0.0\n",
    "\n",
    "        self.last_wip_value = 0\n",
    "        self.last_wip_time = self.env.now\n",
    "        self.cumulative_wip_area = 0.0\n",
    "        self.current_interval_wip_area = 0.0\n",
    "\n",
    "        self.next_scheduled_arrival_time = float('inf')\n",
    "\n",
    "        self.env.process(self._run_job_arrivals())\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action: int) -> tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        jobs_completed_before_step = self.jobs_completed_this_episode\n",
    "        tardiness_before_step = self.total_tardiness_this_episode\n",
    "        earliness_before_step = self.total_earliness_this_episode\n",
    "\n",
    "        self.current_interval_wip_area = 0.0\n",
    "        self._update_wip_area()\n",
    "\n",
    "        if action == 1:\n",
    "            self._release_job_from_psp()\n",
    "            self._update_wip_area()\n",
    "\n",
    "        time_to_advance = self.agent_decision_interval\n",
    "        if self.next_scheduled_arrival_time < self.env.now + self.agent_decision_interval:\n",
    "            time_to_advance = self.next_scheduled_arrival_time - self.env.now\n",
    "\n",
    "        time_to_advance = max(1e-9, time_to_advance) # to make sure that the simulation will proceed, even if for a very short time\n",
    "        self.env.run(until=self.env.now + time_to_advance)\n",
    "\n",
    "        self._update_wip_area()\n",
    "\n",
    "        if self.env.now >= self.next_scheduled_arrival_time - 1e-9:\n",
    "            self.next_scheduled_arrival_time = float('inf')\n",
    "\n",
    "        avg_wip_for_reward_interval = self.current_interval_wip_area / time_to_advance if time_to_advance > 0 else 0.0\n",
    "\n",
    "        jobs_completed_in_interval = self.jobs_completed_this_episode - jobs_completed_before_step\n",
    "        tardiness_in_interval = self.total_tardiness_this_episode - tardiness_before_step\n",
    "        earliness_in_interval = self.total_earliness_this_episode - earliness_before_step\n",
    "\n",
    "        self.running_throughput.update(np.array([[jobs_completed_in_interval]]))\n",
    "        self.running_tardiness.update(np.array([[tardiness_in_interval]]))\n",
    "        self.running_wip.update(np.array([[avg_wip_for_reward_interval]]))\n",
    "\n",
    "        reward = self._calculate_reward(\n",
    "            jobs_completed_in_interval,\n",
    "            tardiness_in_interval,\n",
    "            avg_wip_for_reward_interval\n",
    "        )\n",
    "\n",
    "        terminated = self.env.now >= self.episode_duration\n",
    "        truncated = False\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if terminated:\n",
    "            self.current_wip = len([job for job in self.jobs if job.in_system and not job.done])\n",
    "            self._update_wip_area()\n",
    "            average_wip_for_episode = self.cumulative_wip_area / self.episode_duration if self.episode_duration > 0 else 0\n",
    "            info[\"average_wip_for_episode\"] = average_wip_for_episode\n",
    "            info[\"total_simulation_time_minutes\"] = self.env.now\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        presence_job_top = 1.0 if self.pre_shop_pool else 0.0\n",
    "\n",
    "        job_top_proc_time = 0.0\n",
    "        job_top_urgency = 0.0\n",
    "        if presence_job_top == 1.0:\n",
    "            job_top = self.pre_shop_pool[0]\n",
    "            job_top_proc_time = job_top.total_processing_time\n",
    "            job_top_urgency = job_top.due_date - self.env.now\n",
    "\n",
    "        raw_wip = self.current_wip\n",
    "\n",
    "        machine_workloads = []\n",
    "        for machine in self.machines:\n",
    "            current_machine_workload = 0.0\n",
    "\n",
    "            if machine.job_on_machine is not None:\n",
    "                time_spent_on_op = self.env.now - machine.job_start_time\n",
    "                remaining_op_time = machine.job_on_machine.process_time - time_spent_on_op\n",
    "                current_machine_workload += max(0, remaining_op_time)\n",
    "\n",
    "            for request in machine.queue:\n",
    "                job_in_queue = request.associated_job\n",
    "                current_machine_workload += job_in_queue.process_time\n",
    "\n",
    "            machine_workloads.append(current_machine_workload)\n",
    "\n",
    "        return np.array([\n",
    "            presence_job_top,\n",
    "            job_top_proc_time,\n",
    "            job_top_urgency,\n",
    "            float(raw_wip),\n",
    "            *machine_workloads\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _run_job_arrivals(self) -> ProcessGenerator:\n",
    "        while True:\n",
    "            timeout_inter_arrival = self.inter_arrival_time_distribution()\n",
    "            self.next_scheduled_arrival_time = self.env.now + timeout_inter_arrival\n",
    "            yield self.env.timeout(timeout_inter_arrival)\n",
    "\n",
    "            weight = self.families_distribution()\n",
    "            if weight <= 0.1:\n",
    "                family = 1\n",
    "            elif weight <= 0.62:\n",
    "                family = 2\n",
    "            else:\n",
    "                family = 3\n",
    "\n",
    "            due_date_offset = self.due_dates_distribution()\n",
    "\n",
    "            family_routing_distr = self.routing_distribution[family]\n",
    "            family_routing_prob = self.routing_prob[family]\n",
    "\n",
    "            job_routing = []\n",
    "            processing_times = []\n",
    "            for i in range(6):\n",
    "                if family_routing_distr[i]() <= family_routing_prob[i]:\n",
    "                    job_routing.append(self.machines[i])\n",
    "                    processing_times.append(self.processing_time_per_family_distribution[family-1]())\n",
    "\n",
    "            job = Job(\n",
    "                env=self.env,\n",
    "                routing=job_routing,\n",
    "                arrival_time=self.env.now,\n",
    "                process_times=processing_times,\n",
    "                due_date=(self.env.now + due_date_offset),\n",
    "                idx=self.idx_counter,\n",
    "                family=\"F{}\".format(family),\n",
    "                completion_callback=self._job_completed_callback\n",
    "            )\n",
    "\n",
    "            self.idx_counter += 1\n",
    "            self.jobs.append(job)\n",
    "            self.pre_shop_pool.append(job)\n",
    "\n",
    "    def _release_job_from_psp(self):\n",
    "        if self.pre_shop_pool:\n",
    "            job_to_release = self.pre_shop_pool.pop(0)\n",
    "\n",
    "            self._update_wip_area()\n",
    "\n",
    "            job_to_release.in_system = True\n",
    "            self.current_wip += 1\n",
    "            self.env.process(job_to_release.main())\n",
    "\n",
    "\n",
    "    def _job_completed_callback(self, job: Job):\n",
    "        self._update_wip_area()\n",
    "        job.in_system = False\n",
    "        self.current_wip -= 1\n",
    "        self.jobs_completed_this_episode += 1\n",
    "        self.total_tardiness_this_episode += job.tardiness\n",
    "        self.total_earliness_this_episode += job.earliness\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, jobs_completed_in_interval: int, tardiness_in_interval: float, avg_wip_for_reward_interval: int) -> float:\n",
    "\n",
    "        norm_throughput = (jobs_completed_in_interval - self.running_throughput.mean[0]) / (np.sqrt(self.running_throughput.var[0]) + 1e-8)\n",
    "        norm_tardiness = (tardiness_in_interval - self.running_tardiness.mean[0]) / (np.sqrt(self.running_tardiness.var[0]) + 1e-8)\n",
    "        norm_wip = (avg_wip_for_reward_interval - self.running_wip.mean[0]) / (np.sqrt(self.running_wip.var[0]) + 1e-8)\n",
    "\n",
    "        norm_throughput = np.clip(norm_throughput, -10, 10)\n",
    "        norm_tardiness = np.clip(norm_tardiness, -10, 10)\n",
    "        norm_wip = np.clip(norm_wip, -10, 10)\n",
    "        norm_earliness = 0\n",
    "\n",
    "        reward = (self.reward_weights['throughput'] * norm_throughput) + \\\n",
    "                 (self.reward_weights['tardiness_penalty'] * norm_tardiness) + \\\n",
    "                 (self.reward_weights['wip_penalty'] * norm_wip) + \\\n",
    "                 (self.reward_weights['earliness_penalty'] * norm_earliness)\n",
    "        return reward\n",
    "\n",
    "    def _get_info(self) -> dict:\n",
    "        info = {\n",
    "            \"current_time\": self.env.now,\n",
    "            \"jobs_in_psp\": len(self.pre_shop_pool),\n",
    "            \"jobs_in_system_wip\": self.current_wip,\n",
    "            \"jobs_completed_episode\": self.jobs_completed_this_episode,\n",
    "            \"total_tardiness_episode\": self.total_tardiness_this_episode,\n",
    "            \"total_earliness_episode\": self.total_earliness_this_episode\n",
    "        }\n",
    "        return info\n"
   ],
   "id": "94d85b5be668137c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def make_env():\n",
    "    return SystemEnv(\n",
    "        inter_arrival_time_distribution=lambda: random.expovariate(lambd=0.65),\n",
    "        processing_time_per_family_distribution=[\n",
    "            lambda: random.gammavariate(2,2),\n",
    "            lambda: random.gammavariate(4,0.5),\n",
    "            lambda: random.gammavariate(6,1/6)\n",
    "        ],\n",
    "        families_distribution=lambda: random.random(),\n",
    "        routing_distribution={\n",
    "            1: [lambda: random.random(), lambda: random.random(), lambda: random.random(),lambda: random.random(),lambda: random.random(),lambda: random.random()],\n",
    "            2: [lambda: random.random(), lambda: random.random(), lambda: random.random(),lambda: random.random(),lambda: random.random(),lambda: random.random()],\n",
    "            3: [lambda: random.random(), lambda: random.random(), lambda: random.random(),lambda: random.random(),lambda: random.random(),lambda: random.random()]\n",
    "        },\n",
    "        routing_prob={\n",
    "            1: [1,1,0,1,1,1],\n",
    "            2: [0.8, 0.8, 1, 0.8, 0.8, 0.75],\n",
    "            3: [0,0,1,0,0,0.75]\n",
    "        },\n",
    "        due_dates_distribution=lambda: random.uniform(30,50),\n",
    "        reward_weights={\n",
    "            'throughput': 2.0,\n",
    "            'wip_penalty': -0.5,\n",
    "            'tardiness_penalty': -2\n",
    "        }\n",
    "    )\n",
    "\n",
    "num_envs = 4\n",
    "seed = 42\n",
    "\n",
    "vec_env = make_vec_env(make_env, n_envs=num_envs, seed=seed)\n",
    "vec_env = VecNormalize(\n",
    "    vec_env,\n",
    "    norm_obs=True,\n",
    "    norm_reward=True,\n",
    "    clip_obs=10,\n",
    "    clip_reward=10,\n",
    "    gamma=0.99\n",
    ")\n",
    "\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=50000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=(1, \"step\"),\n",
    "    gradient_steps=1,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    target_update_interval=1000,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "print(\"Starting DQN training...\")\n",
    "total_timesteps = 1000000\n",
    "model.learn(total_timesteps=total_timesteps, log_interval=10)\n",
    "print(\"DQN training finished\")\n",
    "\n",
    "model.save(\"production_system_agent_dqn_final\")\n",
    "vec_env.save(\"vec_normalize_stats_dqn_final.pkl\")"
   ],
   "id": "c7c58e5729a234d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nStarting evaluation phase...\")\n",
    "\n",
    "eval_env = make_vec_env(make_env, n_envs=1, seed=seed + 1)\n",
    "\n",
    "eval_env = VecNormalize.load(\"vec_normalize_stats_dqn_final.pkl\", eval_env)\n",
    "\n",
    "eval_env.training = False\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "model = DQN.load(\"production_system_agent_dqn_final\", env=eval_env)\n",
    "\n",
    "num_eval_episodes = 10\n",
    "episode_rewards = []\n",
    "episode_tardiness = []\n",
    "episode_throughput = []\n",
    "episode_avg_wip = []\n",
    "\n",
    "episode_hourly_throughput = []\n",
    "episode_hourly_tardiness = []\n",
    "episode_hourly_wip = []\n",
    "\n",
    "for episode in range(num_eval_episodes):\n",
    "    obs = eval_env.reset()\n",
    "    print(f\"Type of reset_result: {type(obs)}\")\n",
    "    print(f\"Value of reset_result: {obs}\")\n",
    "\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done_status_array, info = eval_env.step(action)\n",
    "        print(obs, reward, done_status_array, info)\n",
    "        total_reward += reward[0]\n",
    "        if done_status_array[0]:\n",
    "            episode_rewards.append(total_reward)\n",
    "            episode_tardiness.append(info[0][\"total_tardiness_episode\"])\n",
    "            episode_throughput.append(info[0][\"jobs_completed_episode\"])\n",
    "            if \"average_wip_for_episode\" in info[0]:\n",
    "                episode_avg_wip.append(info[0][\"average_wip_for_episode\"])\n",
    "            else:\n",
    "                print(\"Warning: 'average_wip_for_episode' not found in info dict for this episode.\")\n",
    "\n",
    "            current_simulation_time_minutes = info[0].get(\"total_simulation_time_minutes\", 60 * 24 * 7)\n",
    "            episode_duration_hours = current_simulation_time_minutes / 60.0\n",
    "            if episode_duration_hours > 0:\n",
    "                hourly_throughput = info[0][\"jobs_completed_episode\"] / episode_duration_hours\n",
    "                episode_hourly_throughput.append(hourly_throughput)\n",
    "\n",
    "                hourly_tardiness = info[0][\"total_tardiness_episode\"] / episode_duration_hours\n",
    "                episode_hourly_tardiness.append(hourly_tardiness)\n",
    "\n",
    "                episode_hourly_wip.append(info[0][\"average_wip_for_episode\"])\n",
    "            else:\n",
    "                episode_hourly_throughput.append(0.0)\n",
    "                episode_hourly_tardiness.append(0.0)\n",
    "                episode_hourly_wip.append(0.0)\n",
    "            done = True\n",
    "\n",
    "print(f\"\\nEvaluation on {num_eval_episodes} episodes:\")\n",
    "print(f\"Average Reward per episode: {np.mean(episode_rewards):.2f}\")\n",
    "print(f\"Average total tardiness per episode: {np.mean(episode_tardiness):.2f}\")\n",
    "print(f\"Average Throughput per episode: {np.mean(episode_throughput):.2f}\")\n",
    "if episode_avg_wip:\n",
    "    print(f\"Average WIP per episode: {np.mean(episode_avg_wip):.2f}\")\n",
    "\n",
    "print(\"\\n--- Hourly Metrics (Averages per Episode) ---\")\n",
    "if episode_hourly_throughput:\n",
    "    print(f\"Average Hourly Throughput: {np.mean(episode_hourly_throughput):.2f} jobs/hour\")\n",
    "else:\n",
    "    print(\"No data for Average Hourly Throughput.\")\n",
    "\n",
    "if episode_hourly_tardiness:\n",
    "    print(f\"Average Hourly Tardiness: {np.mean(episode_hourly_tardiness):.2f} units/hour\")\n",
    "else:\n",
    "    print(\"No data for Average Hourly Tardiness.\")\n",
    "\n",
    "if episode_hourly_wip:\n",
    "    print(f\"Average Hourly WIP (mean jobs in system): {np.mean(episode_hourly_wip):.2f} jobs\")\n",
    "else:\n",
    "    print(\"No data for Average Hourly WIP.\")\n",
    "\n",
    "eval_env.close()\n",
    "vec_env.close()\n"
   ],
   "id": "40cfa7ccf77b7a52",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
